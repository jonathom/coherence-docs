{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8c77618-0c59-4b21-91f6-fa29bc405c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "# load bursts we want to process\n",
    "bursts_to_process = gpd.read_file(\"/data/users/Public/jonathanbahlmann/coherence-docs/src/processing.geojson\")\n",
    "\n",
    "# go per scene to process, dissolve to extract ids\n",
    "combinations = bursts_to_process.dissolve([\"id\", \"subswath\", \"ref_scene\"], as_index = False)\n",
    "# print(combinations.head())\n",
    "array_of_frames = [v for k, v in combinations.loc[:,[\"id\", \"ref_scene\", \"subswath\", \"sce_min_burst\", \"sce_max_burst\", \"ref_min_burst\", \"ref_max_burst\", \"processing_status\", \"path\", \"ref_path\"]].groupby(['id', \"ref_scene\"])]\n",
    "#print(array_of_frames)\n",
    "\n",
    "# print(combinations.head())\n",
    "# combinations.loc[:,[\"id\", \"ref_scene\", \"subswath\", \"sce_min_burst\", \"sce_max_burst\", \"ref_min_burst\", \"ref_max_burst\", \"processing_status\"]]\n",
    "# combinations.groupby([\"id\", \"ref_scene\"])\n",
    "\n",
    "# each scene and ref_scene could be added together with merge, but not across ref_scenes probably\n",
    "\n",
    "#print(array_of_frames.head())\n",
    "\n",
    "# bursts_to_process.iloc[0].loc[[\"subswath\", \"burst\", \"id\", \"path\", \"sensor\", \"polarizations\", \"mode\", \"orbit_dicrection\", \"rel_orbit\", \"regular_burst_pattern\", \"ref_scene\", \"ref_min_burst\", \"ref_max_burst\", \"processing_status\", \"sce_min_burst\", \"sce_max_burst\"]].to_dict()\n",
    "\n",
    "\n",
    "# out into the spark parallelizer: array of dicts? what does it need: path to scene, which subswaths to process with which ref scenes, maybe like path..., subswaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22cdd764-74cb-4830-b742-25f52815a72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d00545ca-73a4-462b-9688-f27bf1598f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IW1': {'min_sce': 1.0, 'max_sce': 5.0, 'min_ref': 5.0, 'max_ref': 9.0}, 'IW2': {'min_sce': 1.0, 'max_sce': 5.0, 'min_ref': 5.0, 'max_ref': 9.0}, 'IW3': {'min_sce': 1.0, 'max_sce': 5.0, 'min_ref': 5.0, 'max_ref': 9.0}}\n",
      "[PROCESSING IW1]:  {'min_sce': 1.0, 'max_sce': 5.0, 'min_ref': 5.0, 'max_ref': 9.0}\n",
      "[PROCESSING IW2]:  {'min_sce': 1.0, 'max_sce': 5.0, 'min_ref': 5.0, 'max_ref': 9.0}\n",
      "[PROCESSING IW3]:  {'min_sce': 1.0, 'max_sce': 5.0, 'min_ref': 5.0, 'max_ref': 9.0}\n",
      "[['Read', 'TOPSAR-Split (5)'], ['Apply-Orbit-File (5)'], ['Read', 'TOPSAR-Split (3)'], ['Apply-Orbit-File (3)'], ['Read', 'TOPSAR-Split'], ['Apply-Orbit-File'], ['Read (2)', 'TOPSAR-Split (6)'], ['Apply-Orbit-File (6)'], ['Back-Geocoding (3)'], ['TOPSAR-Deburst (3)'], ['Read (2)', 'TOPSAR-Split (4)'], ['Apply-Orbit-File (4)'], ['Back-Geocoding (2)'], ['TOPSAR-Deburst (2)'], ['Read (2)', 'TOPSAR-Split (2)'], ['Apply-Orbit-File (2)'], ['Back-Geocoding'], ['TOPSAR-Deburst'], ['TOPSAR-Merge', 'Write']]\n",
      "{'IW1': {'min_sce': 6.0, 'max_sce': 9.0, 'min_ref': 1.0, 'max_ref': 4.0}, 'IW2': {'min_sce': 6.0, 'max_sce': 9.0, 'min_ref': 1.0, 'max_ref': 4.0}, 'IW3': {'min_sce': 6.0, 'max_sce': 9.0, 'min_ref': 1.0, 'max_ref': 4.0}}\n",
      "[PROCESSING IW1]:  {'min_sce': 6.0, 'max_sce': 9.0, 'min_ref': 1.0, 'max_ref': 4.0}\n",
      "[PROCESSING IW2]:  {'min_sce': 6.0, 'max_sce': 9.0, 'min_ref': 1.0, 'max_ref': 4.0}\n",
      "[PROCESSING IW3]:  {'min_sce': 6.0, 'max_sce': 9.0, 'min_ref': 1.0, 'max_ref': 4.0}\n",
      "[['Read', 'TOPSAR-Split (5)'], ['Apply-Orbit-File (5)'], ['Read', 'TOPSAR-Split (3)'], ['Apply-Orbit-File (3)'], ['Read', 'TOPSAR-Split'], ['Apply-Orbit-File'], ['Read (2)', 'TOPSAR-Split (6)'], ['Apply-Orbit-File (6)'], ['Back-Geocoding (3)'], ['TOPSAR-Deburst (3)'], ['Read (2)', 'TOPSAR-Split (4)'], ['Apply-Orbit-File (4)'], ['Back-Geocoding (2)'], ['TOPSAR-Deburst (2)'], ['Read (2)', 'TOPSAR-Split (2)'], ['Apply-Orbit-File (2)'], ['Back-Geocoding'], ['TOPSAR-Deburst'], ['TOPSAR-Merge', 'Write']]\n",
      "{'IW1': {'min_sce': 6.0, 'max_sce': 9.0, 'min_ref': 1.0, 'max_ref': 4.0}, 'IW2': {'min_sce': 6.0, 'max_sce': 9.0, 'min_ref': 1.0, 'max_ref': 4.0}, 'IW3': {'min_sce': 6.0, 'max_sce': 9.0, 'min_ref': 1.0, 'max_ref': 4.0}}\n",
      "[PROCESSING IW1]:  {'min_sce': 6.0, 'max_sce': 9.0, 'min_ref': 1.0, 'max_ref': 4.0}\n",
      "[PROCESSING IW2]:  {'min_sce': 6.0, 'max_sce': 9.0, 'min_ref': 1.0, 'max_ref': 4.0}\n",
      "[PROCESSING IW3]:  {'min_sce': 6.0, 'max_sce': 9.0, 'min_ref': 1.0, 'max_ref': 4.0}\n",
      "[['Read', 'TOPSAR-Split (5)'], ['Apply-Orbit-File (5)'], ['Read', 'TOPSAR-Split (3)'], ['Apply-Orbit-File (3)'], ['Read', 'TOPSAR-Split'], ['Apply-Orbit-File'], ['Read (2)', 'TOPSAR-Split (6)'], ['Apply-Orbit-File (6)'], ['Back-Geocoding (3)'], ['TOPSAR-Deburst (3)'], ['Read (2)', 'TOPSAR-Split (4)'], ['Apply-Orbit-File (4)'], ['Back-Geocoding (2)'], ['TOPSAR-Deburst (2)'], ['Read (2)', 'TOPSAR-Split (2)'], ['Apply-Orbit-File (2)'], ['Back-Geocoding'], ['TOPSAR-Deburst'], ['TOPSAR-Merge', 'Write']]\n",
      "{'IW1': {'min_sce': 4.0, 'max_sce': 5.0, 'min_ref': 8.0, 'max_ref': 9.0}, 'IW2': {'min_sce': 4.0, 'max_sce': 5.0, 'min_ref': 8.0, 'max_ref': 9.0}, 'IW3': {'min_sce': 5.0, 'max_sce': 5.0, 'min_ref': 9.0, 'max_ref': 9.0}}\n",
      "[PROCESSING IW1]:  {'min_sce': 4.0, 'max_sce': 5.0, 'min_ref': 8.0, 'max_ref': 9.0}\n",
      "[PROCESSING IW2]:  {'min_sce': 4.0, 'max_sce': 5.0, 'min_ref': 8.0, 'max_ref': 9.0}\n",
      "[PROCESSING IW3]:  {'min_sce': 5.0, 'max_sce': 5.0, 'min_ref': 9.0, 'max_ref': 9.0}\n",
      "[['Read', 'TOPSAR-Split (5)'], ['Apply-Orbit-File (5)'], ['Read', 'TOPSAR-Split (3)'], ['Apply-Orbit-File (3)'], ['Read', 'TOPSAR-Split'], ['Apply-Orbit-File'], ['Read (2)', 'TOPSAR-Split (6)'], ['Apply-Orbit-File (6)'], ['Back-Geocoding (3)'], ['TOPSAR-Deburst (3)'], ['Read (2)', 'TOPSAR-Split (4)'], ['Apply-Orbit-File (4)'], ['Back-Geocoding (2)'], ['TOPSAR-Deburst (2)'], ['Read (2)', 'TOPSAR-Split (2)'], ['Apply-Orbit-File (2)'], ['Back-Geocoding'], ['TOPSAR-Deburst'], ['TOPSAR-Merge', 'Write']]\n"
     ]
    }
   ],
   "source": [
    "from pyroSAR.snap.auxil import parse_recipe, parse_node\n",
    "from pyroSAR.snap.auxil import gpt\n",
    "from pyroSAR.snap.auxil import groupbyWorkers\n",
    "\n",
    "output_dir = \"/data/users/Public/jonathanbahlmann/spark_results/\"\n",
    "\n",
    "continueOnFailAOF = True\n",
    "\n",
    "# this iteration is done by spark\n",
    "for df in array_of_frames:\n",
    "    # this needs to be put into each worker node\n",
    "    name = str(df.iloc[0][\"id\"]) + \"_\" + df.iloc[0][\"ref_scene\"] + \"_\" + str(int(df.iloc[0][\"sce_min_burst\"])) + \"_\" + str(int(df.iloc[0][\"sce_max_burst\"]))\n",
    "    workflow_filename = name + \".xml\"\n",
    "    out_filename = name + \"_res\"\n",
    "    \n",
    "    # each of these frames that I get here are similar to the pyroXX_workflow.py\n",
    "    processing_dict = {\"IW1\": {\"min_sce\": None, \"max_sce\": None, \"min_ref\": None, \"max_ref\": None}, \n",
    "                       \"IW2\": {\"min_sce\": None, \"max_sce\": None, \"min_ref\": None, \"max_ref\": None}, \n",
    "                       \"IW3\": {\"min_sce\": None, \"max_sce\": None, \"min_ref\": None, \"max_ref\": None}}\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        swath = row[\"subswath\"]\n",
    "        processing_dict[swath][\"min_sce\"] = row[\"sce_min_burst\"]\n",
    "        processing_dict[swath][\"max_sce\"] = row[\"sce_max_burst\"]\n",
    "        processing_dict[swath][\"min_ref\"] = row[\"ref_min_burst\"]\n",
    "        processing_dict[swath][\"max_ref\"] = row[\"ref_max_burst\"]\n",
    "\n",
    "    print(processing_dict)\n",
    "    \n",
    "    workflow = parse_recipe('blank')\n",
    "\n",
    "    # reference\n",
    "    read = parse_node(\"Read\")\n",
    "    read.parameters[\"file\"] = df[\"ref_path\"]\n",
    "    read.parameters[\"formatName\"] = \"SENTINEL-1\"\n",
    "    workflow.insert_node(read)\n",
    "\n",
    "    # secondary\n",
    "    read2 = parse_node(\"Read\")\n",
    "    read2.parameters[\"file\"] = df[\"path\"]\n",
    "    read2.parameters[\"formatName\"] = \"SENTINEL-1\"\n",
    "    workflow.insert_node(read2)\n",
    "\n",
    "    merge_list = []\n",
    "    \n",
    "    # check IW1, empty?\n",
    "    if processing_dict[\"IW1\"][\"max_sce\"] is None or processing_dict[\"IW1\"][\"max_ref\"] is None:\n",
    "        print(\"[PROCESSING IW1]: No bursts needed from IW1\")\n",
    "    else:\n",
    "        print(\"[PROCESSING IW1]: \", processing_dict[\"IW1\"])\n",
    "        # TopSAR Split\n",
    "        split = parse_node(\"TOPSAR-Split\")\n",
    "        split.parameters[\"subswath\"] = \"IW1\"\n",
    "        split.parameters[\"selectedPolarisations\"] = [\"VV\"]\n",
    "        split.parameters[\"firstBurstIndex\"] = processing_dict[\"IW1\"][\"min_ref\"]\n",
    "        split.parameters[\"lastBurstIndex\"] = processing_dict[\"IW1\"][\"max_ref\"]\n",
    "        workflow.insert_node(split, before = read.id, resetSuccessorSource = False)\n",
    "\n",
    "        # TopSAR Split 2\n",
    "        split2 = parse_node(\"TOPSAR-Split\")\n",
    "        split2.parameters[\"subswath\"] = \"IW1\"\n",
    "        split2.parameters[\"selectedPolarisations\"] = [\"VV\"]\n",
    "        split2.parameters[\"firstBurstIndex\"] = processing_dict[\"IW1\"][\"min_sce\"]\n",
    "        split2.parameters[\"lastBurstIndex\"] = processing_dict[\"IW1\"][\"max_sce\"]\n",
    "        workflow.insert_node(split2, before = read2.id, resetSuccessorSource = False)\n",
    "\n",
    "        # apply orbit file 1\n",
    "        aof = parse_node(\"Apply-Orbit-File\")\n",
    "        aof.parameters[\"orbitType\"] = \"Sentinel Restituted (Auto Download)\"\n",
    "        aof.parameters[\"polyDegree\"] = 3\n",
    "        aof.parameters[\"continueOnFail\"] = continueOnFailAOF\n",
    "        workflow.insert_node(aof, before = split.id)\n",
    "\n",
    "        # apply orbit file 2\n",
    "        aof2 = parse_node(\"Apply-Orbit-File\")\n",
    "        aof2.parameters[\"orbitType\"] = \"Sentinel Restituted (Auto Download)\"\n",
    "        aof2.parameters[\"polyDegree\"] = 3\n",
    "        aof2.parameters[\"continueOnFail\"] = continueOnFailAOF\n",
    "        workflow.insert_node(aof2, before = split2.id)\n",
    "\n",
    "        # Back-Geocoding\n",
    "        geocode = parse_node(\"Back-Geocoding\")\n",
    "        geocode.parameters[\"demName\"] = \"SRTM 1Sec HGT\"\n",
    "        workflow.insert_node(geocode, before = [aof.id, aof2.id])\n",
    "\n",
    "        # deburst\n",
    "        deb = parse_node(\"TOPSAR-Deburst\")\n",
    "        workflow.insert_node(deb, before = geocode.id)\n",
    "\n",
    "        merge_list.append(deb.id)\n",
    "    # check IW2, empty?\n",
    "    if processing_dict[\"IW2\"][\"max_sce\"] is None or processing_dict[\"IW2\"][\"max_ref\"] is None:\n",
    "        print(\"[PROCESSING IW2]: No bursts needed from IW2\")\n",
    "    else:\n",
    "        print(\"[PROCESSING IW2]: \", processing_dict[\"IW2\"])\n",
    "        # TopSAR Split\n",
    "        split3 = parse_node(\"TOPSAR-Split\")\n",
    "        split3.parameters[\"subswath\"] = \"IW2\"\n",
    "        split3.parameters[\"selectedPolarisations\"] = [\"VV\"]\n",
    "        split3.parameters[\"firstBurstIndex\"] = processing_dict[\"IW2\"][\"min_ref\"]\n",
    "        split3.parameters[\"lastBurstIndex\"] = processing_dict[\"IW2\"][\"max_ref\"]\n",
    "        workflow.insert_node(split3, before = read.id, resetSuccessorSource = False)\n",
    "\n",
    "        # TopSAR Split 2\n",
    "        split4 = parse_node(\"TOPSAR-Split\")\n",
    "        split4.parameters[\"subswath\"] = \"IW2\"\n",
    "        split4.parameters[\"selectedPolarisations\"] = [\"VV\"]\n",
    "        split4.parameters[\"firstBurstIndex\"] = processing_dict[\"IW2\"][\"min_sce\"]\n",
    "        split4.parameters[\"lastBurstIndex\"] = processing_dict[\"IW2\"][\"max_sce\"]\n",
    "        workflow.insert_node(split4, before = read2.id, resetSuccessorSource = False)\n",
    "\n",
    "        # apply orbit file 1\n",
    "        aof3 = parse_node(\"Apply-Orbit-File\")\n",
    "        aof3.parameters[\"orbitType\"] = \"Sentinel Restituted (Auto Download)\"\n",
    "        aof3.parameters[\"polyDegree\"] = 3\n",
    "        aof3.parameters[\"continueOnFail\"] = continueOnFailAOF\n",
    "        workflow.insert_node(aof3, before = split3.id)\n",
    "\n",
    "        # apply orbit file 2\n",
    "        aof4 = parse_node(\"Apply-Orbit-File\")\n",
    "        aof4.parameters[\"orbitType\"] = \"Sentinel Restituted (Auto Download)\"\n",
    "        aof4.parameters[\"polyDegree\"] = 3\n",
    "        aof4.parameters[\"continueOnFail\"] = continueOnFailAOF\n",
    "        workflow.insert_node(aof4, before = split4.id)\n",
    "\n",
    "        # Back-Geocoding\n",
    "        geocode2 = parse_node(\"Back-Geocoding\")\n",
    "        geocode2.parameters[\"demName\"] = \"SRTM 1Sec HGT\"\n",
    "        workflow.insert_node(geocode2, before = [aof3.id, aof4.id])\n",
    "\n",
    "        # deburst\n",
    "        deb2 = parse_node(\"TOPSAR-Deburst\")\n",
    "        workflow.insert_node(deb2, before = geocode2.id)\n",
    "\n",
    "        merge_list.append(deb2.id)\n",
    "    # check IW3, empty?\n",
    "    if processing_dict[\"IW3\"][\"max_sce\"] is None or processing_dict[\"IW3\"][\"max_ref\"] is None:\n",
    "        print(\"[PROCESSING IW3]: No bursts needed from IW3\")\n",
    "    else:\n",
    "        print(\"[PROCESSING IW3]: \", processing_dict[\"IW3\"])\n",
    "        # TopSAR Split\n",
    "        split5 = parse_node(\"TOPSAR-Split\")\n",
    "        split5.parameters[\"subswath\"] = \"IW3\"\n",
    "        split5.parameters[\"selectedPolarisations\"] = [\"VV\"]\n",
    "        split5.parameters[\"firstBurstIndex\"] = processing_dict[\"IW3\"][\"min_ref\"]\n",
    "        split5.parameters[\"lastBurstIndex\"] = processing_dict[\"IW3\"][\"max_ref\"]\n",
    "        workflow.insert_node(split5, before = read.id, resetSuccessorSource = False)\n",
    "\n",
    "        # TopSAR Split 2\n",
    "        split6 = parse_node(\"TOPSAR-Split\")\n",
    "        split6.parameters[\"subswath\"] = \"IW3\"\n",
    "        split6.parameters[\"selectedPolarisations\"] = [\"VV\"]\n",
    "        split6.parameters[\"firstBurstIndex\"] = processing_dict[\"IW3\"][\"min_sce\"]\n",
    "        split6.parameters[\"lastBurstIndex\"] = processing_dict[\"IW3\"][\"max_sce\"]\n",
    "        workflow.insert_node(split6, before = read2.id, resetSuccessorSource = False)\n",
    "\n",
    "        # apply orbit file 1\n",
    "        aof5 = parse_node(\"Apply-Orbit-File\")\n",
    "        aof5.parameters[\"orbitType\"] = \"Sentinel Restituted (Auto Download)\"\n",
    "        aof5.parameters[\"polyDegree\"] = 3\n",
    "        aof5.parameters[\"continueOnFail\"] = continueOnFailAOF\n",
    "        workflow.insert_node(aof5, before = split5.id)\n",
    "\n",
    "        # apply orbit file 2\n",
    "        aof6 = parse_node(\"Apply-Orbit-File\")\n",
    "        aof6.parameters[\"orbitType\"] = \"Sentinel Restituted (Auto Download)\"\n",
    "        aof6.parameters[\"polyDegree\"] = 3\n",
    "        aof6.parameters[\"continueOnFail\"] = continueOnFailAOF\n",
    "        workflow.insert_node(aof6, before = split6.id)\n",
    "\n",
    "        # Back-Geocoding\n",
    "        geocode3 = parse_node(\"Back-Geocoding\")\n",
    "        geocode3.parameters[\"demName\"] = \"SRTM 1Sec HGT\"\n",
    "        workflow.insert_node(geocode3, before = [aof5.id, aof6.id])\n",
    "\n",
    "        # deburst\n",
    "        deb3 = parse_node(\"TOPSAR-Deburst\")\n",
    "        workflow.insert_node(deb3, before = geocode3.id)\n",
    "\n",
    "        merge_list.append(deb3.id)\n",
    "        \n",
    "    # hope this works even with only one subswath\n",
    "    merge = parse_node(\"TOPSAR-Merge\")\n",
    "    workflow.insert_node(merge, before = merge_list)\n",
    "                        \n",
    "    write = parse_node(\"Write\")\n",
    "    write.parameters[\"file\"] = out_filename\n",
    "    write.parameters[\"formatName\"] = \"BEAM-DIMAP\"\n",
    "    workflow.insert_node(write, before = merge.id)\n",
    "                        \n",
    "    workflow.write(output_dir + workflow_filename)\n",
    "                        \n",
    "    groups = groupbyWorkers(output_dir + workflow_filename, n=1)\n",
    "    print(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3f02ab1-2718-4806-9fbb-f54b8949f908",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-fb1c9edf599d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"subswaths\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"IW1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ref\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"subswaths\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"hi\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "dic[\"path\"]\n",
    "dic[\"subswaths\"][0][\"IW1\"][\"ref\"]\n",
    "dic[\"subswaths\"][2] = {\"test\": \"hi\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59e25d90-b9fa-4194-a52f-eb958a265e76",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'set' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-ddb1b755cf81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fg\"\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"fr\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'set' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "d = {\"path\", \"fg\" }\n",
    "d[\"path\"] = \"fr\"\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5db931f5-3e5c-4a36-b6b4-910151b70dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/MTDA/CGS_S1/CGS_S1_SLC_L1/IW/DV/2020/12/30\n",
      "/data/MTDA/CGS_S1/CGS_S1_SLC_L1/IW/DV/2020/12/31\n",
      "/data/MTDA/CGS_S1/CGS_S1_SLC_L1/IW/DV/2021/01/02\n",
      "/data/MTDA/CGS_S1/CGS_S1_SLC_L1/IW/DV/2021/01/01\n"
     ]
    }
   ],
   "source": [
    "start_date = \"2020/12/31\"\n",
    "#str(root / start_date.strftime(\"%Y/%m/%d\"))\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import glob\n",
    "start_date = datetime.datetime.strptime(\"2020/12/30\", \"%Y/%m/%d\")\n",
    "end_date = datetime.datetime.strptime(\"2021/01/03\", \"%Y/%m/%d\")\n",
    "\n",
    "root = Path(\"/data/MTDA/CGS_S1/CGS_S1_SLC_L1/IW/DV/\")\n",
    "start_dir = str(root / start_date.strftime(\"%Y/%m/%d\"))\n",
    "end_dir = str(root / end_date.strftime(\"%Y/%m/%d\"))\n",
    "\n",
    "list_of_products = []\n",
    "for year in range(start_date.year, end_date.year + 1):\n",
    "    year_path = root / str(year)\n",
    "    for day_dir in year_path.glob(\"[01][0123456789]/[0123][0123456789]\"):\n",
    "        if start_dir <= str(day_dir) < end_dir:\n",
    "            print(day_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1eea61c-fdb2-492c-b50e-442c71ea5321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/data/users/Public/jonathanbahlmann/coherence-docs')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.util import list_products_by_time\n",
    "start = \"2017/06/28\"\n",
    "end = \"2017/07/03\"\n",
    "path = \"/data/MTDA/CGS_S1/CGS_S1_SLC_L1/IW/DV/\"\n",
    "expected = 18 # usually 18\n",
    "products = list_products_by_time(start, end, path)\n",
    "# products\n",
    "\n",
    "import pathlib\n",
    " \n",
    "path = pathlib.Path('.')\n",
    "full_path = path.absolute()\n",
    " \n",
    "my_path = full_path.as_posix()\n",
    "full_path\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acad2611-c6f0-40f1-bf88-0dd2cfb71f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/geopandas/base.py:35: UserWarning: GeoSeries crs mismatch: {'init': 'epsg:4326'} and EPSG:4326\n",
      "  right.crs))\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-50b9b65a3ee3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mreferences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_for_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene_gpd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_gpd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mreferences\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.util import search_for_reference, create_gpd_for_scene\n",
    "import geopandas as gpd\n",
    "\n",
    "scene_gpd = create_gpd_for_scene(path = \"/data/MTDA/CGS_S1/CGS_S1_SLC_L1/IW/DV/2021/04/17/S1B_IW_SLC__1SDV_20210417T174054_20210417T174130_026510_032A4B_DA8B/S1B_IW_SLC__1SDV_20210417T174054_20210417T174130_026510_032A4B_DA8B.zip\")\n",
    "ref_gpd = gpd.read_file(\"src/reference_bursts.geojson\")\n",
    "references = search_for_reference(scene_gpd, ref_gpd)\n",
    "print(references)\n",
    "assert references == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b542323d-30dd-4a17-8bfb-b1155f782901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
