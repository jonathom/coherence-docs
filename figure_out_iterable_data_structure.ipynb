{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d8c77618-0c59-4b21-91f6-fa29bc405c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     id ref_scene subswath  sce_min_burst  sce_max_burst  ref_min_burst  \\\n",
      "0  136C      A2E3      IW1            1.0            9.0            1.0   \n",
      "1  136C      A2E3      IW2            3.0            9.0            3.0   \n",
      "\n",
      "   ref_max_burst  processing_status  \\\n",
      "0            9.0                0.0   \n",
      "1            9.0                0.0   \n",
      "\n",
      "                                                path  \\\n",
      "0  /data/MTDA/CGS_S1/CGS_S1_SLC_L1/IW/DV/2021/09/...   \n",
      "1  /data/MTDA/CGS_S1/CGS_S1_SLC_L1/IW/DV/2021/09/...   \n",
      "\n",
      "                                            ref_path  \n",
      "0  /data/MTDA/CGS_S1/CGS_S1_SLC_L1/IW/DV/2021/10/...  \n",
      "1  /data/MTDA/CGS_S1/CGS_S1_SLC_L1/IW/DV/2021/10/...  ]\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "# load bursts we want to process\n",
    "bursts_to_process = gpd.read_file(\"/data/users/Public/jonathanbahlmann/coherence-docs/src/processing.geojson\")\n",
    "\n",
    "# go per scene to process, dissolve to extract ids\n",
    "combinations = bursts_to_process.dissolve([\"id\", \"subswath\", \"ref_scene\"], as_index = False)\n",
    "combinations = combinations.loc[combinations[\"sce_min_burst\"] != combinations[\"sce_max_burst\"],:]\n",
    "# print(combinations)\n",
    "array_of_frames = [v for k, v in combinations.loc[:,[\"id\", \"ref_scene\", \"subswath\", \"sce_min_burst\", \"sce_max_burst\", \"ref_min_burst\", \"ref_max_burst\", \"processing_status\", \"path\", \"ref_path\"]].groupby(['id', \"ref_scene\"])]\n",
    "#for row in range(0,4):\n",
    " #   print(type(len(array_of_frames[row])))\n",
    "print(array_of_frames)\n",
    "\n",
    "# print(combinations.head())\n",
    "# combinations.loc[:,[\"id\", \"ref_scene\", \"subswath\", \"sce_min_burst\", \"sce_max_burst\", \"ref_min_burst\", \"ref_max_burst\", \"processing_status\"]]\n",
    "# combinations.groupby([\"id\", \"ref_scene\"])\n",
    "\n",
    "# each scene and ref_scene could be added together with merge, but not across ref_scenes probably\n",
    "\n",
    "#print(array_of_frames.head())\n",
    "\n",
    "# bursts_to_process.iloc[0].loc[[\"subswath\", \"burst\", \"id\", \"path\", \"sensor\", \"polarizations\", \"mode\", \"orbit_dicrection\", \"rel_orbit\", \"regular_burst_pattern\", \"ref_scene\", \"ref_min_burst\", \"ref_max_burst\", \"processing_status\", \"sce_min_burst\", \"sce_max_burst\"]].to_dict()\n",
    "\n",
    "\n",
    "# out into the spark parallelizer: array of dicts? what does it need: path to scene, which subswaths to process with which ref scenes, maybe like path..., subswaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "22cdd764-74cb-4830-b742-25f52815a72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/gpt', '-e', '-Dsnap.userdir=.', '-Duser.home=.', '-Dsnap.log.level=DEBUG']\n"
     ]
    }
   ],
   "source": [
    "cmd = [\"/gpt\", '-e']\n",
    "ls = ['-Dsnap.userdir=.', '-Duser.home=.', '-Dsnap.log.level=DEBUG']\n",
    "if isinstance(ls, list):\n",
    "    cmd.extend(ls)\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d00545ca-73a4-462b-9688-f27bf1598f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IW1': {'min_sce': 1, 'max_sce': 9, 'min_ref': 1, 'max_ref': 9}, 'IW2': {'min_sce': 3, 'max_sce': 9, 'min_ref': 3, 'max_ref': 9}, 'IW3': {'min_sce': None, 'max_sce': None, 'min_ref': None, 'max_ref': None}}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ref_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ref_path'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-eee66b730d89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"file\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ref_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ref_path\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"formatName\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"SENTINEL-1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mworkflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1416\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    803\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_label_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0msection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0;31m# we have yielded a scalar ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no slices here, handle elsewhere\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mxs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   3735\u001b[0m             \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3736\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3737\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ref_path'"
     ]
    }
   ],
   "source": [
    "from pyroSAR.snap.auxil import parse_recipe, parse_node\n",
    "from pyroSAR.snap.auxil import gpt\n",
    "from pyroSAR.snap.auxil import groupbyWorkers\n",
    "\n",
    "output_dir = \"/data/users/Public/jonathanbahlmann/spark_results/\"\n",
    "\n",
    "continueOnFailAOF = True\n",
    "\n",
    "# this iteration is done by spark\n",
    "for df in array_of_frames:\n",
    "    # this needs to be put into each worker node\n",
    "    name = str(df.iloc[0][\"id\"]) + \"_\" + df.iloc[0][\"ref_scene\"] + \"_\" + str(int(df.iloc[0][\"sce_min_burst\"])) + \"_\" + str(int(df.iloc[0][\"sce_max_burst\"]))\n",
    "    workflow_filename = name + \".xml\"\n",
    "    out_filename = name + \"_res\"\n",
    "    \n",
    "    # each of these frames that I get here are similar to the pyroXX_workflow.py\n",
    "    processing_dict = {\"IW1\": {\"min_sce\": None, \"max_sce\": None, \"min_ref\": None, \"max_ref\": None}, \n",
    "                       \"IW2\": {\"min_sce\": None, \"max_sce\": None, \"min_ref\": None, \"max_ref\": None}, \n",
    "                       \"IW3\": {\"min_sce\": None, \"max_sce\": None, \"min_ref\": None, \"max_ref\": None}}\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        swath = row[\"subswath\"]\n",
    "        processing_dict[swath][\"min_sce\"] = int(row[\"sce_min_burst\"])\n",
    "        processing_dict[swath][\"max_sce\"] = int(row[\"sce_max_burst\"])\n",
    "        processing_dict[swath][\"min_ref\"] = int(row[\"ref_min_burst\"])\n",
    "        processing_dict[swath][\"max_ref\"] = int(row[\"ref_max_burst\"])\n",
    "\n",
    "    print(processing_dict)\n",
    "    \n",
    "    workflow = parse_recipe('blank')\n",
    "\n",
    "    # reference\n",
    "    read = parse_node(\"Read\")\n",
    "    read.parameters[\"file\"] = df[\"ref_path\"]\n",
    "    print(df.loc[\"ref_path\",:])\n",
    "    read.parameters[\"formatName\"] = \"SENTINEL-1\"\n",
    "    workflow.insert_node(read)\n",
    "\n",
    "    # secondary\n",
    "    read2 = parse_node(\"Read\")\n",
    "    read2.parameters[\"file\"] = df[\"path\"]\n",
    "    read2.parameters[\"formatName\"] = \"SENTINEL-1\"\n",
    "    workflow.insert_node(read2)\n",
    "\n",
    "    merge_list = []\n",
    "    \n",
    "    # check IW1, empty?\n",
    "    if processing_dict[\"IW1\"][\"max_sce\"] is None or processing_dict[\"IW1\"][\"max_ref\"] is None:\n",
    "        print(\"[PROCESSING IW1]: No bursts needed from IW1\")\n",
    "    else:\n",
    "        print(\"[PROCESSING IW1]: \", processing_dict[\"IW1\"])\n",
    "        # TopSAR Split\n",
    "        split = parse_node(\"TOPSAR-Split\")\n",
    "        split.parameters[\"subswath\"] = \"IW1\"\n",
    "        split.parameters[\"selectedPolarisations\"] = [\"VV\"]\n",
    "        split.parameters[\"firstBurstIndex\"] = processing_dict[\"IW1\"][\"min_ref\"]\n",
    "        split.parameters[\"lastBurstIndex\"] = processing_dict[\"IW1\"][\"max_ref\"]\n",
    "        workflow.insert_node(split, before = read.id, resetSuccessorSource = False)\n",
    "\n",
    "        # TopSAR Split 2\n",
    "        split2 = parse_node(\"TOPSAR-Split\")\n",
    "        split2.parameters[\"subswath\"] = \"IW1\"\n",
    "        split2.parameters[\"selectedPolarisations\"] = [\"VV\"]\n",
    "        split2.parameters[\"firstBurstIndex\"] = processing_dict[\"IW1\"][\"min_sce\"]\n",
    "        split2.parameters[\"lastBurstIndex\"] = processing_dict[\"IW1\"][\"max_sce\"]\n",
    "        workflow.insert_node(split2, before = read2.id, resetSuccessorSource = False)\n",
    "\n",
    "        # apply orbit file 1\n",
    "        aof = parse_node(\"Apply-Orbit-File\")\n",
    "        aof.parameters[\"orbitType\"] = \"Sentinel Restituted (Auto Download)\"\n",
    "        aof.parameters[\"polyDegree\"] = 3\n",
    "        aof.parameters[\"continueOnFail\"] = continueOnFailAOF\n",
    "        workflow.insert_node(aof, before = split.id)\n",
    "\n",
    "        # apply orbit file 2\n",
    "        aof2 = parse_node(\"Apply-Orbit-File\")\n",
    "        aof2.parameters[\"orbitType\"] = \"Sentinel Restituted (Auto Download)\"\n",
    "        aof2.parameters[\"polyDegree\"] = 3\n",
    "        aof2.parameters[\"continueOnFail\"] = continueOnFailAOF\n",
    "        workflow.insert_node(aof2, before = split2.id)\n",
    "\n",
    "        # Back-Geocoding\n",
    "        geocode = parse_node(\"Back-Geocoding\")\n",
    "        geocode.parameters[\"demName\"] = \"SRTM 1Sec HGT\"\n",
    "        workflow.insert_node(geocode, before = [aof.id, aof2.id])\n",
    "\n",
    "        # deburst\n",
    "        deb = parse_node(\"TOPSAR-Deburst\")\n",
    "        workflow.insert_node(deb, before = geocode.id)\n",
    "\n",
    "        merge_list.append(deb.id)\n",
    "    # check IW2, empty?\n",
    "    if processing_dict[\"IW2\"][\"max_sce\"] is None or processing_dict[\"IW2\"][\"max_ref\"] is None:\n",
    "        print(\"[PROCESSING IW2]: No bursts needed from IW2\")\n",
    "    else:\n",
    "        print(\"[PROCESSING IW2]: \", processing_dict[\"IW2\"])\n",
    "        # TopSAR Split\n",
    "        split3 = parse_node(\"TOPSAR-Split\")\n",
    "        split3.parameters[\"subswath\"] = \"IW2\"\n",
    "        split3.parameters[\"selectedPolarisations\"] = [\"VV\"]\n",
    "        split3.parameters[\"firstBurstIndex\"] = processing_dict[\"IW2\"][\"min_ref\"]\n",
    "        split3.parameters[\"lastBurstIndex\"] = processing_dict[\"IW2\"][\"max_ref\"]\n",
    "        workflow.insert_node(split3, before = read.id, resetSuccessorSource = False)\n",
    "\n",
    "        # TopSAR Split 2\n",
    "        split4 = parse_node(\"TOPSAR-Split\")\n",
    "        split4.parameters[\"subswath\"] = \"IW2\"\n",
    "        split4.parameters[\"selectedPolarisations\"] = [\"VV\"]\n",
    "        split4.parameters[\"firstBurstIndex\"] = processing_dict[\"IW2\"][\"min_sce\"]\n",
    "        split4.parameters[\"lastBurstIndex\"] = processing_dict[\"IW2\"][\"max_sce\"]\n",
    "        workflow.insert_node(split4, before = read2.id, resetSuccessorSource = False)\n",
    "\n",
    "        # apply orbit file 1\n",
    "        aof3 = parse_node(\"Apply-Orbit-File\")\n",
    "        aof3.parameters[\"orbitType\"] = \"Sentinel Restituted (Auto Download)\"\n",
    "        aof3.parameters[\"polyDegree\"] = 3\n",
    "        aof3.parameters[\"continueOnFail\"] = continueOnFailAOF\n",
    "        workflow.insert_node(aof3, before = split3.id)\n",
    "\n",
    "        # apply orbit file 2\n",
    "        aof4 = parse_node(\"Apply-Orbit-File\")\n",
    "        aof4.parameters[\"orbitType\"] = \"Sentinel Restituted (Auto Download)\"\n",
    "        aof4.parameters[\"polyDegree\"] = 3\n",
    "        aof4.parameters[\"continueOnFail\"] = continueOnFailAOF\n",
    "        workflow.insert_node(aof4, before = split4.id)\n",
    "\n",
    "        # Back-Geocoding\n",
    "        geocode2 = parse_node(\"Back-Geocoding\")\n",
    "        geocode2.parameters[\"demName\"] = \"SRTM 1Sec HGT\"\n",
    "        workflow.insert_node(geocode2, before = [aof3.id, aof4.id])\n",
    "\n",
    "        # deburst\n",
    "        deb2 = parse_node(\"TOPSAR-Deburst\")\n",
    "        workflow.insert_node(deb2, before = geocode2.id)\n",
    "\n",
    "        merge_list.append(deb2.id)\n",
    "    # check IW3, empty?\n",
    "    if processing_dict[\"IW3\"][\"max_sce\"] is None or processing_dict[\"IW3\"][\"max_ref\"] is None:\n",
    "        print(\"[PROCESSING IW3]: No bursts needed from IW3\")\n",
    "    else:\n",
    "        print(\"[PROCESSING IW3]: \", processing_dict[\"IW3\"])\n",
    "        # TopSAR Split\n",
    "        split5 = parse_node(\"TOPSAR-Split\")\n",
    "        split5.parameters[\"subswath\"] = \"IW3\"\n",
    "        split5.parameters[\"selectedPolarisations\"] = [\"VV\"]\n",
    "        split5.parameters[\"firstBurstIndex\"] = processing_dict[\"IW3\"][\"min_ref\"]\n",
    "        split5.parameters[\"lastBurstIndex\"] = processing_dict[\"IW3\"][\"max_ref\"]\n",
    "        workflow.insert_node(split5, before = read.id, resetSuccessorSource = False)\n",
    "\n",
    "        # TopSAR Split 2\n",
    "        split6 = parse_node(\"TOPSAR-Split\")\n",
    "        split6.parameters[\"subswath\"] = \"IW3\"\n",
    "        split6.parameters[\"selectedPolarisations\"] = [\"VV\"]\n",
    "        split6.parameters[\"firstBurstIndex\"] = processing_dict[\"IW3\"][\"min_sce\"]\n",
    "        split6.parameters[\"lastBurstIndex\"] = processing_dict[\"IW3\"][\"max_sce\"]\n",
    "        workflow.insert_node(split6, before = read2.id, resetSuccessorSource = False)\n",
    "\n",
    "        # apply orbit file 1\n",
    "        aof5 = parse_node(\"Apply-Orbit-File\")\n",
    "        aof5.parameters[\"orbitType\"] = \"Sentinel Restituted (Auto Download)\"\n",
    "        aof5.parameters[\"polyDegree\"] = 3\n",
    "        aof5.parameters[\"continueOnFail\"] = continueOnFailAOF\n",
    "        workflow.insert_node(aof5, before = split5.id)\n",
    "\n",
    "        # apply orbit file 2\n",
    "        aof6 = parse_node(\"Apply-Orbit-File\")\n",
    "        aof6.parameters[\"orbitType\"] = \"Sentinel Restituted (Auto Download)\"\n",
    "        aof6.parameters[\"polyDegree\"] = 3\n",
    "        aof6.parameters[\"continueOnFail\"] = continueOnFailAOF\n",
    "        workflow.insert_node(aof6, before = split6.id)\n",
    "\n",
    "        # Back-Geocoding\n",
    "        geocode3 = parse_node(\"Back-Geocoding\")\n",
    "        geocode3.parameters[\"demName\"] = \"SRTM 1Sec HGT\"\n",
    "        workflow.insert_node(geocode3, before = [aof5.id, aof6.id])\n",
    "\n",
    "        # deburst\n",
    "        deb3 = parse_node(\"TOPSAR-Deburst\")\n",
    "        workflow.insert_node(deb3, before = geocode3.id)\n",
    "\n",
    "        merge_list.append(deb3.id)\n",
    "        \n",
    "    # hope this works even with only one subswath\n",
    "    merge = parse_node(\"TOPSAR-Merge\")\n",
    "    workflow.insert_node(merge, before = merge_list)\n",
    "                        \n",
    "    write = parse_node(\"Write\")\n",
    "    write.parameters[\"file\"] = out_filename\n",
    "    write.parameters[\"formatName\"] = \"BEAM-DIMAP\"\n",
    "    workflow.insert_node(write, before = merge.id)\n",
    "                        \n",
    "    workflow.write(output_dir + workflow_filename)\n",
    "                        \n",
    "    groups = groupbyWorkers(output_dir + workflow_filename, n=1)\n",
    "    print(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3f02ab1-2718-4806-9fbb-f54b8949f908",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-fb1c9edf599d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"subswaths\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"IW1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ref\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"subswaths\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"hi\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "dic[\"path\"]\n",
    "dic[\"subswaths\"][0][\"IW1\"][\"ref\"]\n",
    "dic[\"subswaths\"][2] = {\"test\": \"hi\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59e25d90-b9fa-4194-a52f-eb958a265e76",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'set' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-ddb1b755cf81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fg\"\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"fr\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'set' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "d = {\"path\", \"fg\" }\n",
    "d[\"path\"] = \"fr\"\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5db931f5-3e5c-4a36-b6b4-910151b70dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/MTDA/CGS_S1/CGS_S1_SLC_L1/IW/DV/2020/12/30\n",
      "/data/MTDA/CGS_S1/CGS_S1_SLC_L1/IW/DV/2020/12/31\n",
      "/data/MTDA/CGS_S1/CGS_S1_SLC_L1/IW/DV/2021/01/02\n",
      "/data/MTDA/CGS_S1/CGS_S1_SLC_L1/IW/DV/2021/01/01\n"
     ]
    }
   ],
   "source": [
    "start_date = \"2020/12/31\"\n",
    "#str(root / start_date.strftime(\"%Y/%m/%d\"))\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import glob\n",
    "start_date = datetime.datetime.strptime(\"2020/12/30\", \"%Y/%m/%d\")\n",
    "end_date = datetime.datetime.strptime(\"2021/01/03\", \"%Y/%m/%d\")\n",
    "\n",
    "root = Path(\"/data/MTDA/CGS_S1/CGS_S1_SLC_L1/IW/DV/\")\n",
    "start_dir = str(root / start_date.strftime(\"%Y/%m/%d\"))\n",
    "end_dir = str(root / end_date.strftime(\"%Y/%m/%d\"))\n",
    "\n",
    "list_of_products = []\n",
    "for year in range(start_date.year, end_date.year + 1):\n",
    "    year_path = root / str(year)\n",
    "    for day_dir in year_path.glob(\"[01][0123456789]/[0123][0123456789]\"):\n",
    "        if start_dir <= str(day_dir) < end_dir:\n",
    "            print(day_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1eea61c-fdb2-492c-b50e-442c71ea5321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/data/users/Public/jonathanbahlmann/coherence-docs')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.util import list_products_by_time\n",
    "start = \"2017/06/28\"\n",
    "end = \"2017/07/03\"\n",
    "path = \"/data/MTDA/CGS_S1/CGS_S1_SLC_L1/IW/DV/\"\n",
    "expected = 18 # usually 18\n",
    "products = list_products_by_time(start, end, path)\n",
    "# products\n",
    "\n",
    "import pathlib\n",
    " \n",
    "path = pathlib.Path('.')\n",
    "full_path = path.absolute()\n",
    " \n",
    "my_path = full_path.as_posix()\n",
    "full_path\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acad2611-c6f0-40f1-bf88-0dd2cfb71f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/geopandas/base.py:35: UserWarning: GeoSeries crs mismatch: {'init': 'epsg:4326'} and EPSG:4326\n",
      "  right.crs))\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-50b9b65a3ee3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mreferences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_for_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene_gpd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_gpd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mreferences\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.util import search_for_reference, create_gpd_for_scene\n",
    "import geopandas as gpd\n",
    "\n",
    "scene_gpd = create_gpd_for_scene(path = \"/data/MTDA/CGS_S1/CGS_S1_SLC_L1/IW/DV/2021/04/17/S1B_IW_SLC__1SDV_20210417T174054_20210417T174130_026510_032A4B_DA8B/S1B_IW_SLC__1SDV_20210417T174054_20210417T174130_026510_032A4B_DA8B.zip\")\n",
    "ref_gpd = gpd.read_file(\"src/reference_bursts.geojson\")\n",
    "references = search_for_reference(scene_gpd, ref_gpd)\n",
    "print(references)\n",
    "assert references == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b542323d-30dd-4a17-8bfb-b1155f782901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
